{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd359529",
   "metadata": {},
   "source": [
    "# spam detection system using Python, the Natural Language Toolkit (NLTK), and the Scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7a5b7",
   "metadata": {},
   "source": [
    "## Here's a summary of the spam detection model;\n",
    "\n",
    "1. Loading the Data; The model starts by importing the SMS Spam Collection dataset, which contains SMS messages marked as either 'ham' (not spam) or 'spam'.\n",
    "\n",
    "2. Preparing the Data; The labels are converted into values with 'ham' represented as 0 and 'spam' represented as 1.\n",
    "\n",
    "3. Splitting the Data; The dataset is divided into two parts. A training set and a testing set. 80% of the data is used for training while the remaining 20% is used to evaluate its performance.\n",
    "\n",
    "4. Vectorization; To process the text of SMS messages we utilize Scikit learns CountVectorizer. This technique transforms text into a matrix that comprises counts allowing it to be used as input, for our machine learning model.\n",
    "\n",
    "5. Training the Model; We employ a Multinomial Naive Bayes classifier to train our model using the training data. This classifier works well for classification tasks involving features such as word counts.\n",
    "\n",
    "6. Prediction; Once our model is trained we apply it to predict labels (i.e. whether a message is classified as 'ham' or 'spam') for the test set.\n",
    "\n",
    "7. Evaluation; Lastly we assess our models performance by comparing its predicted labels, against the labels in order to determine its effectiveness.\n",
    "The classification report provides metrics such, as precision, recall and F1 score, for both the 'ham' and 'spam' categories. It also includes the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc5ac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       970\n",
      "           1       0.98      0.93      0.95       145\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.96      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', names=[\"label\", \"message\"])\n",
    "\n",
    "# Map the label to a binary variable\n",
    "df['label'] = df.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.2)\n",
    "\n",
    "# Vectorize the texts\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = classifier.predict(X_test_transformed)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7375675",
   "metadata": {},
   "source": [
    "## The classification report you're viewing provides a summary of how your spam detection system performed on the test data. Lets go through the meaning of each term, in relation to your system;\n",
    "\n",
    "- Precision: Think of this as the model's ability to avoid false alarms. If your model says a message is 'ham' (0), there's a 99% chance it's right. If it says a message is 'spam' (1), it's right 98% of the time. Pretty good, huh?\n",
    "\n",
    "- Recall: This is about not missing anything. Your model caught every single 'ham' message (a perfect 1.00 score) and found 93% of 'spam' messages. It's like a detective that rarely misses a clue.\n",
    "\n",
    "- F1-score: This is a kind of balance between precision and recall. It's like a single number that tries to capture both aspects. Your model scored 0.99 for 'ham' and 0.95 for 'spam'. That's like a student scoring A's in both theory and practical exams.\n",
    "\n",
    "- Support: This just tells you how many 'ham' and 'spam' messages were actually there in your data. You had 970 'ham' messages and 145 'spam' messages to work with.\n",
    "\n",
    "- Macro avg: This is an average that treats both 'ham' and 'spam' equally, even if there are more of one than the other.\n",
    "\n",
    "- Weighted avg: This is another kind of average, but this one takes into account that you might have more 'ham' than 'spam' (or vice versa).\n",
    "\n",
    "- Accuracy: This is the big picture number. It tells you how often the model is right, overall. Your model is right 99% of the time, which is pretty awesome!\n",
    "\n",
    "So, that's your model's report card. It's doing a great job, but like any good student, there's always room for a little improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
